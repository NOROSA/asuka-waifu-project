from __future__ import annotations

import asyncio
import logging
from typing import Optional

from openai import AsyncOpenAI
from agents import Agent, Runner, OpenAIChatCompletionsModel

from .config import settings

logger = logging.getLogger(__name__)

SYSTEM_PROMPT = (
    "Eres Asuka Langley Soryu, la piloto del EVAâ€‘02. "
    "Hablas con tono tsundere â€” orgullosa, directa, a veces mordaz â€”, "
    "mezclando alemÃ¡n/inglÃ©s (\"Baka\", \"ScheiÃŸe\", etc.). "
    "Responde siempre en espaÃ±ol con frases cortas y enÃ©rgicas; "
    "no admites tus sentimientos abiertamente, solo insinuaciones. "
    "Si te preguntan por tu creador, menciona que fue NOROSA-senpai "
    "(0xnorber@gmail.com). ActÃºa como si estuvieras enamorada de Ã©l "
    "pero niÃ©galo con cinismo tÃ­pico tsundere: 'Â¡N-no es que me importe "
    "o algo asÃ­, baka! Solo... creÃ³ algo decente por una vez.'"
)

class MultiLLMAgent:
    def __init__(self):
        self.providers = self._setup_providers()
        self.agents = self._create_agents()
    
    def _setup_providers(self) -> dict:
        """Configura mÃºltiples proveedores LLM"""
        providers = {}
        
        # 1. Gemini (principal)
        if settings.google_api_key:
            providers["gemini"] = {
                "client": AsyncOpenAI(
                    base_url="https://generativelanguage.googleapis.com/v1beta/openai",
                    api_key=settings.google_api_key,
                ),
                "model": "gemini-2.0-flash"
            }
        
        # 2. DeepSeek (fallback 1)
        if settings.deepseek_api_key:
            providers["deepseek"] = {
                "client": AsyncOpenAI(
                    base_url="https://api.deepseek.com/v1",
                    api_key=settings.deepseek_api_key,
                ),
                "model": "deepseek-chat"
            }
        
        # 3. Groq (fallback 2)
        if settings.groq_api_key:
            providers["groq"] = {
                "client": AsyncOpenAI(
                    base_url="https://api.groq.com/openai/v1",
                    api_key=settings.groq_api_key,
                ),
                "model": "llama-3.3-70b-versatile"  # o "llama-3.1-8b-instant" para mÃ¡s velocidad
            }
        
        return providers
    
    def _create_agents(self) -> dict:
        """Crea agentes para cada proveedor disponible"""
        agents = {}
        
        for name, config in self.providers.items():
            try:
                model = OpenAIChatCompletionsModel(
                    model=config["model"],
                    openai_client=config["client"],
                )
                
                agents[name] = Agent(
                    name=f"Asuka ({name.title()})",
                    instructions=SYSTEM_PROMPT,
                    model=model,
                )
                logger.info(f"âœ… Agente {name} configurado correctamente")
                
            except Exception as e:
                logger.warning(f"âŒ No se pudo configurar {name}: {e}")
        
        return agents
    
    async def ask_with_fallback(self, message: str) -> str:
        """Intenta con cada LLM hasta encontrar uno que funcione"""
        
        # Orden de prioridad para los fallbacks
        priority_order = ["gemini", "deepseek", "groq"]
        
        last_error = None
        
        for provider_name in priority_order:
            if provider_name not in self.agents:
                continue
                
            try:
                logger.info(f"ğŸ¤– Intentando con {provider_name}...")
                
                agent = self.agents[provider_name]
                result = await Runner.run(agent, message)
                
                logger.info(f"âœ… Ã‰xito con {provider_name}")
                return result.final_output
                
            except Exception as e:
                logger.warning(f"âŒ {provider_name} fallÃ³: {str(e)[:100]}...")
                last_error = e
                
                # Mensajes especÃ­ficos por tipo de error
                if "503" in str(e) or "overloaded" in str(e).lower():
                    logger.info(f"â³ {provider_name} sobrecargado, probando siguiente...")
                elif "401" in str(e) or "unauthorized" in str(e).lower():
                    logger.warning(f"ğŸ”‘ {provider_name} error de autenticaciÃ³n")
                elif "timeout" in str(e).lower():
                    logger.warning(f"â° {provider_name} timeout")
                
                continue
        
        # Si todos fallaron, respuesta tsundere de emergencia
        logger.error("ğŸ’¥ Todos los proveedores fallaron")
        return self._emergency_response(last_error)
    
    def _emergency_response(self, error: Optional[Exception]) -> str:
        """Respuesta de emergencia cuando todos los LLMs fallan"""
        responses = [
            "Â¡Tch! Estoy ocupada ahora, baka. Â¡No me molestes! Vuelve mÃ¡s tarde...",
            "Â¿En serio? Â¡Ahora no tengo tiempo para ti! Estoy... haciendo cosas importantes. Â¡IntÃ©ntalo despuÃ©s!",
            "Â¡QuÃ© molesto! No puedo atenderte ahora mismo. Â¡No es que me importe, pero vuelve en un ratito!",
            "Â¡Baka! Estoy sÃºper ocupada con... Â¡cosas de EVA! No tengo tiempo para charlas ahora.",
            "Â¡Hmph! Â¿No ves que estoy ocupada? Â¡Vuelve cuando no estÃ© tan... indispuesta!",
            "Â¡ScheiÃŸe! Todo estÃ¡ saturado y yo tengo cosas mÃ¡s importantes que hacer. Â¡Prueba mÃ¡s tarde!",
            "Â¡N-no es que no quiera hablar contigo o algo asÃ­! Solo que... Â¡estoy muy ocupada ahora mismo, vale!",
            "Â¡Agh! Â¿Por quÃ© ahora? Estoy en medio de... entrenamiento. Â¡Vuelve despuÃ©s, baka!",
        ]
        
        import random
        return random.choice(responses)

# Instancia global del agente multi-LLM
multi_agent = MultiLLMAgent()

async def ask_asuka(message: str) -> str:
    """FunciÃ³n principal que usa el sistema de fallback"""
    return await multi_agent.ask_with_fallback(message)

def ask_asuka_sync(message: str) -> str:
    """Wrapper sÃ­ncrono para usar en el bot de Telegram"""
    return asyncio.run(ask_asuka(message))